<div class=WordSection1 style='layout-grid:15.6pt'>


<p class=MsoNormal align=center style='text-align:center'><a name="OLE_LINK2"></a><a
name="OLE_LINK1"><span style='mso-bookmark:OLE_LINK2'><span style='font-size:
36.0pt;font-family:华文隶书'>哈尔滨工业大学<span lang=EN-US><o:p></o:p></span></span></span></a></p>


<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span lang=EN-US style='font-size:26.0pt;
mso-fareast-font-family:黑体'>&lt;&lt;</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>模式识别与深度学习</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'>&gt;&gt;<o:p></o:p></span></b></span></span></p>
<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span style='font-size:26.0pt;font-family:
黑体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri'>实验</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'>6 </span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>实验报告</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'><o:p></o:p></span></b></span></span></p>
<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span style='font-size:26.0pt;font-family:
黑体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri'>（2020</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'></span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>春季学期）</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'><o:p></o:p></span></b></span></span></p>






<div align=center>
<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none;mso-border-top-alt:1.5pt;
 mso-border-left-alt:.5pt;mso-border-bottom-alt:1.5pt;mso-border-right-alt:
 .5pt;mso-border-color-alt:windowtext;mso-border-style-alt:solid;mso-yfti-tbllook:
 480;mso-padding-alt:0cm 5.4pt 0cm 5.4pt;mso-border-insideh:.5pt solid windowtext;
 mso-border-insidev:.5pt solid windowtext'>
 <tr style='mso-yfti-irow:0;mso-yfti-firstrow:yes'>
  <td width=189 style='width:141.5pt;border:solid windowtext 1.0pt;border-top:
  solid windowtext 1.5pt;mso-border-alt:solid windowtext .5pt;mso-border-top-alt:
  solid windowtext 1.5pt;background:#CCECFF;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 align=right style='text-align:right;text-indent:32.15pt'><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span style='font-size:16.0pt;font-family:
  楷体_GB2312;mso-hansi-font-family:"Times New Roman"'>成员<span lang=EN-US
  style='color:black;mso-color-alt:windowtext'>1</span><span style='color:black;
  mso-color-alt:windowtext'>：</span><span lang=EN-US><o:p></o:p></span></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
  <td width=239 style='width:178.9pt;border-top:solid windowtext 1.5pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  mso-border-left-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;
  mso-border-top-alt:solid windowtext 1.5pt;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 style='text-align:justify;text-justify:inter-ideograph;
  text-indent:32.15pt'><span style='mso-bookmark:OLE_LINK1'><span
  style='mso-bookmark:OLE_LINK2'><b style='mso-bidi-font-weight:normal'><span
  lang=EN-US style='font-size:16.0pt;font-family:"Times New Roman",serif;
  mso-fareast-font-family:楷体_GB2312'>1170500913 </span></b></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><span
  class=GramE><b style='mso-bidi-font-weight:normal'><span style='font-size:
  16.0pt;font-family:楷体_GB2312;mso-ascii-font-family:"Times New Roman";
  mso-hansi-font-family:"Times New Roman"'>熊健羽</span></b></span></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:16.0pt;
  font-family:"Times New Roman",serif;mso-fareast-font-family:楷体_GB2312'><o:p></o:p></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
 </tr>
 <tr style='mso-yfti-irow:1'>
  <td width=189 style='width:141.5pt;border:solid windowtext 1.0pt;border-top:
  solid windowtext 1.5pt;mso-border-alt:solid windowtext .5pt;mso-border-top-alt:
  solid windowtext 1.5pt;background:#CCECFF;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 align=right style='text-align:right;text-indent:32.15pt'><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span style='font-size:16.0pt;font-family:
  楷体_GB2312;mso-hansi-font-family:"Times New Roman"'>成员<span lang=EN-US
  style='color:black;mso-color-alt:windowtext'>2</span><span style='color:black;
  mso-color-alt:windowtext'>：</span><span lang=EN-US><o:p></o:p></span></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
  <td width=239 style='width:178.9pt;border-top:solid windowtext 1.5pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  mso-border-left-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;
  mso-border-top-alt:solid windowtext 1.5pt;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 style='text-align:justify;text-justify:inter-ideograph;
  text-indent:32.15pt'><span style='mso-bookmark:OLE_LINK1'><span
  style='mso-bookmark:OLE_LINK2'><b style='mso-bidi-font-weight:normal'><span
  lang=EN-US style='font-size:16.0pt;font-family:"Times New Roman",serif;
  mso-fareast-font-family:楷体_GB2312'>1171000520 </span></b></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><span
  class=GramE><b style='mso-bidi-font-weight:normal'><span style='font-size:
  16.0pt;font-family:楷体_GB2312;mso-ascii-font-family:"Times New Roman";
  mso-hansi-font-family:"Times New Roman"'>鲍克勤</span></b></span></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:16.0pt;
  font-family:"Times New Roman",serif;mso-fareast-font-family:楷体_GB2312'><o:p></o:p>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
      <tr style='mso-yfti-irow:0;mso-yfti-firstrow:yes'>
  <td width=189 style='width:141.5pt;border:solid windowtext 1.0pt;border-top:
  solid windowtext 1.5pt;mso-border-alt:solid windowtext .5pt;mso-border-top-alt:
  solid windowtext 1.5pt;background:#CCECFF;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 align=right style='text-align:right;text-indent:32.15pt'><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span style='font-size:16.0pt;font-family:
  楷体_GB2312;mso-hansi-font-family:"Times New Roman"'>成员<span lang=EN-US
  style='color:black;mso-color-alt:windowtext'>3</span><span style='color:black;
  mso-color-alt:windowtext'>：</span><span lang=EN-US><o:p></o:p></span></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
  <td width=239 style='width:178.9pt;border-top:solid windowtext 1.5pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  mso-border-left-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;
  mso-border-top-alt:solid windowtext 1.5pt;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 style='text-align:justify;text-justify:inter-ideograph;
  text-indent:32.15pt'><span style='mso-bookmark:OLE_LINK1'><span
  style='mso-bookmark:OLE_LINK2'><b style='mso-bidi-font-weight:normal'><span
  lang=EN-US style='font-size:16.0pt;font-family:"Times New Roman",serif;
  mso-fareast-font-family:楷体_GB2312'>1171000622 </span></b></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><span
  class=GramE><b style='mso-bidi-font-weight:normal'><span style='font-size:
  16.0pt;font-family:楷体_GB2312;mso-ascii-font-family:"Times New Roman";
  mso-hansi-font-family:"Times New Roman"'>章金凯</span></b></span></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:16.0pt;
  font-family:"Times New Roman",serif;mso-fareast-font-family:楷体_GB2312'><o:p></o:p></span></b></span></span></p>
  </td>




 



































## 实验目标

在图像处理的任务中，往往需要我们从图像背景中提取出我们需要的重要信息，如我们希望能够从图片上提取出来马这个动物，因为对于这张图片来说马是主要元素，草原只是他的一个背景，在实际生活中往往有很多这样的应用，我们对此十分感兴趣，在查阅过资料以后，我们了解到使用进行前后景分离DOUBLE DIP是一种简单高效的方法，所以我们希望能够自己动手实现double dip并进行前后景分离。

## 成员分工

1170500913熊健羽：

- 调研前后景分离知识，学习DOUBLE DIP，实现DOUBLE DIP核心代码，以及处理数据

1171000520鲍克勤、1171000622章金凯：

- 调研前后景分离知识，学习DOUBLE DIP，训练DOUBLE DIP并调整参数，对网络结构微调

## 实验环境

实验环境为免费的Baidu AI studio。

操作系统：Ubuntu 16.04

CPU：8核

RAM：32G 

GPU：Tesla V100-SXM2

GPU RAM：16G

## 实验准备

### DIP

#### DIP提出

DIP的提出源自于CVPR2018的一篇论文 Deep Image Prior这篇论文主要的观点有如下几点：

- 之前的观点是认为卷积神经网络之所以在各类问题上能够取得好的结果是因为，其能够从大量数据集中学习到真实图片的先验分布，但是本文认为并不是这样的，本文作者说明神经网络这个结构本身就能够提取低层次统计分布，作者展示了一个随机初始化的神经网络就可以用来提取手工设计的先验分布特征，并在一些标准的逆问题上取得较好的效果
- 另一方面作者认为网络的优秀表现并不只来自于其能够从数据中学习到真实的先验分布
- 作者的工作显示了学习过程并不是建立图片数据先验分布所必须的。神经网络这样一种结构能够捕获大量的图片数据的统计量，且这是独立于训练过程的。在图片恢复问题中这种捕获统计量的能力表现的尤为重要，因为图片恢复需要图片数据的先验分布以恢复在图片下采样过程中损失掉的信息。

1. 深度神经网络可以被用于图片生成，将一个随机噪声映射到图片这就相当于从一个随即图片分布中进行采样，而作者在这里将注意力鸡中于受到一个“损坏”图片限制的生成问题，及图片降噪和超分辨率问题。作者，作者使用Unet结构研究未经训练的网络能捕获先验分布的信息的能力
2. 以上的问题都可以表示为如下能量函数，其中E取决于具体的应用场景，x0是对应的noisy，R是一个正则项用来捕捉自然图像的先验分布，本文中将R使用神经网络来捕捉先验分布，即R这项消失，然后替换为另一个神经网络映射的表述形式。

![[公式]](https://www.zhihu.com/equation?tex=x%5E%2A+%3D+%5Cmin%5Climits_%7Bx%7D+E%28x%3Bx_0%29+%2B+R%28x%29+%3C1%3E)

![[公式]](https://www.zhihu.com/equation?tex=x%5E%E2%88%97%3Df_%7B%CE%B8%5E%E2%88%97%7D+%28x_0%29+)

​			其中

![[公式]](https://www.zhihu.com/equation?tex=%CE%B8%5E%E2%88%97%3D%5Carg%5Cmin%5Climits_%CE%B8+E%28f_%CE%B8+%28z%29%3Bx_0+%29)

​			最后作者通过随机初始化参数然后进行梯度下降得到局部最优点，作者发现在迭代过程中每过过一定循环次数扰动随机向量z会在某些实验中得到较好的效果。

​			因为上面的神经网络并没有经过一个数据集的训练，仅仅是针对需要生成的单个图片进行优化，所以这个神经网络的功能更像是一个hand-crafted prior

​			简单来数DIP实现了用一个神经网络结构捕获图像先验知识的能力，这个能力可以应用在多的领域中。

#### DIP结构

![image_1cakn7am410afatj8nbjo01usq7j.png-110kB](http://static.zybuluo.com/303389737/j7rp3u0olud76yl8ur3fismr/image_1cakn7am410afatj8nbjo01usq7j.png)

如图是DIP的结构，作者使用了Encode-Decoder模型，同时在编码解码的对应层中加入了跨层连接，非线性层使用了LReLU结构，上采样的时候使用了双线性的方式，padding不使用补零，最后优化器选择了ADAM优化器

作者首先通过卷积下采样压缩图像信息，使得图像保留较为低级的统计信息，然后再通过卷积上采样重构图像，重构过程中使用跨层连接将丢失掉的图像信息传递到解卷积层，以此来弥补信息的丢失。

### DOUBLE DIP

#### DOUBLE DIP 原理

​	 Double DIP主要介绍了一种图像分割技术，从字面意思来看其实用了多个DIP结构来捕获图像的先验信息，然后对图像进行分割。

​	 文章的主要贡献在于提供胡扯了一种无监督学习进行图像分层的框架 ，使得每个图层内的图像元素是相互独立的并且简单的，Double DIP使用了DIP生成器网络的结构，上文中已经提到单个DIP网络结构足以捕获单个图像的低级统计数据，DIP网络的输入是随机噪声，训练DIP网络以重建单个图像，这种网络用语解决无监督去噪，超分辨率等问题非常有效果，作者观察到使用多个DIP网络来组合重建图像时，这些DIP倾向于分割图像，使得每个DIP输出相对于其他输出是独立而简单的，因此，作者使用多个DIP组合来进行图像分割，作者证明了这种方法适用于各种计算机视觉任务，如：图像去噪、图像和视频分割，水印去除等等。

​		作者在论文中提到，在一张自然图像中，小的 patch 是有很高的重复性的，这个也是很好理解的，纹理或者结构的尺度越小，重复的概率越大，都是一些很小的边界或者平滑区域，同时，根据数据统计发现，自然图像的纹理具有一定的规律和自相似性，同一区域的小 patch 的 empirical entropy 会小于不同区域的 empirical cross-entropy，基于自然图像纹理结构的内在相关性，作者提出了一种无监督的图层分离方法，将多个 DIP 应用于重建一幅融合图像时，这些 DIP 倾向于将这张复合图像进行分离，将一张融合图像，分成两张相对 simple 的图层。

<img src="https://img-blog.csdnimg.cn/20200405093014871.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaW5pYW4xOTg3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:100%;" />

​		作者在文中运用上图作为示例，上图中橙色和蓝色曲线表示单个图层的拟合 loss，绿色曲线表示混合图像的拟合 loss，从中可以看出，单个图层的拟合比混合图像的拟合要容易，这也间接说明了混合图像的熵比单个图层的熵要大，单个图像的纹理结构有更多的自相似性，混合图像破坏了这种内在的相似结构，所以拟合会更困难。所以为了对混合图像进行分离，我们可以假设图像Z是由图像X和Y组成的，我们可以假设X和Y与Z满足简单的线性组合关系，即Z=a * X + (1- a) * Y

​		另一方面，我们希望每个图层内部的相似性要尽可能的高，不同图层之间的相关性要尽可能地低，对此作者提出了如下地损失函数：
$$
Loss = L_{rec}+ \alpha \dot L_{exl} + \beta L_{reg}
$$
其中第一项是重构损失，第二项是互斥损失，让分离出来的图层尽可能地不相关，第三项是一个与任务相关地正则项，用来约束融合地mask地，不同的任务设置的正则项也不相同，比如图像分割中，mask 希望尽可能二值化，而在图像去雾中，mask 希望是平滑而且连续的。训练的时候位每个图层设置一个DIP网络，输入都是随机噪声，期望通过DIP学习到图层先验信息，最后将两个图层通过一个mask进行融合，这个mask本身也是通过一个DIP网络得到的信息。

#### DOUBLE DIP 网络结构

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405092806142.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaW5pYW4xOTg3,size_16,color_FFFFFF,t_70)

网络结构如图，如上文所介绍的，首先生成三个随机噪声一次输入到DIP网络，通过这个DIP网络将噪声一次转化成Mask信息和前景、后景信息，在前后景图像中使用互斥损失希望二者尽可能不同，将三者通过线性组合得到最终重构的图像，重构图像和原图像放在一起计算得到重构损失，最后损失函数还需要根据任务的不同加上一个正则项作为损失。



## 实验过程

### 数据处理

DIP的思想就是，从网络结构本身就是图像的先验，不需要额外的数据集，只需要图像本身，因此不存在训练数据集。

而对图像的读取则是直接使用了PIL库，相关代码如下：

```python
from PIL import Image

seg = Segmentation()
downsample = (64, 48)
input_img = Image.open('./data/zebra.bmp')     # 读取图像
input_img = input_img.resize(downsample)       # 下采样到标准大小
input_img = np.array(input_img).astype(np.float32) / 255    # 归一化，[0, 255] -> [0, 1]
input_img = torch.from_numpy(input_img.transpose(2, 0, 1))
```



### 网络结构实现

#### 单个DIP网络

如前面所述，double DIP的主要网络结构是DIP中的自编码器网络：

![image_1cakn7am410afatj8nbjo01usq7j.png-110kB](http://static.zybuluo.com/303389737/j7rp3u0olud76yl8ur3fismr/image_1cakn7am410afatj8nbjo01usq7j.png)

具体实现如下：

```python
class DIP(nn.Module):
    def __init__(self,
                 down_channels=[8, 16, 32],
                 up_channels=[8, 16, 32],
                 skip_channels=[0, 0, 0],
                 in_channels=2,
                 out_channels=3
                 ):
        super(DIP, self).__init__()
        assert len(down_channels) == len(up_channels)
        self.model = nn.Sequential()
        self.add_module('model', self.model)
        model_temp = self.model
        for i in range(len(down_channels)):

            model_temp.add_module('down_conv1', nn.Conv2d(in_channels=in_channels, out_channels=down_channels[i],
                                 kernel_size=3, stride=2, padding=1))  # 使用stride下采样
            model_temp.add_module('down_bn1', nn.BatchNorm2d(num_features=down_channels[i]))
            model_temp.add_module('down_relu1',nn.LeakyReLU(0.2, inplace=True))
            model_temp.add_module('down_conv2', nn.Conv2d(in_channels=down_channels[i], out_channels=down_channels[i],
                                 kernel_size=3, stride=1, padding=1))
            model_temp.add_module('down_bn2', nn.BatchNorm2d(num_features=down_channels[i]))
            model_temp.add_module('down_relu2', nn.LeakyReLU(0.2, inplace=True))

            deeper = nn.Sequential()   # deeper layers

            skip = nn.Sequential()  # skip connection
            if skip_channels[i] != 0:
                # 该层的skip
                skip.add_module('conv', nn.Conv2d(in_channels=down_channels[i], out_channels=skip_channels[i],
                                   kernel_size=3, stride=1, padding=1))
                skip.add_module('bn', nn.BatchNorm2d(num_features=skip_channels[i]))
                skip.add_module('relu', nn.LeakyReLU(0.2, inplace=True))
                model_temp.add_module('skip+deeper', Concat(skip, deeper))  # 把该层的deeper和skip连接
            else:
                model_temp.add_module('deeper', deeper)
            last_channel = up_channels[i + 1] if i != len(down_channels) - 1 else down_channels[i]

            model_temp.add_module('up_bn1', nn.BatchNorm2d(num_features=last_channel + skip_channels[i]))
            model_temp.add_module('up_conv1', nn.Conv2d(in_channels=last_channel + skip_channels[i],
                                     out_channels=up_channels[i],
                                     kernel_size=3,
                                     padding=1))
            model_temp.add_module('up_bn2', nn.BatchNorm2d(num_features=up_channels[i]))
            model_temp.add_module('up_relu1', nn.LeakyReLU(0.2, inplace=True))
            model_temp.add_module('up_conv2', nn.Conv2d(in_channels=up_channels[i],
                                     out_channels=up_channels[i],
                                     kernel_size=3,
                                     padding=1))
            model_temp.add_module('up_bn3', nn.BatchNorm2d(num_features=up_channels[i]))
            model_temp.add_module('up_relu2', nn.LeakyReLU(0.2, inplace=True))
            model_temp.add_module('up_upsample', nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))
            model_temp = deeper
            in_channels = down_channels[i]
        
        self.model.add_module('last_conv', nn.Conv2d(in_channels=up_channels[0],
                                     out_channels=out_channels,
                                     kernel_size=3,
                                     padding=1))
        self.model.add_module('sigmoid', nn.Sigmoid())

    def forward(self, input):
        return self.model(input)

```

如代码中所示，每一层的通道数，按前向传播顺序设置为[8, 16, 32, 32, 16, 8]。

在自编码器中，涉及到Skip connection，在两个支路的连接处使用的是通道concat（即把两个支路的通道合在一起，得到的通道数是两者之和），因此还需要引入一个Concat结构：

```python
class Concat(nn.Module):
    def __init__(self, model_1, model_2):
        super(Concat, self).__init__()
        self.model_1 = model_1
        self.model_2 = model_2

    def forward(self, input):
        out_1 = self.model_1(input)
        out_2 = self.model_2(input)

        if out_1.shape[2] != out_2.shape[2] or out_1.shape[3] != out_2.shape[3]:  # 如果图像大小不相等
            min_shape_2 = min([out_1.shape[2], out_2.shape[2]])
            min_shape_3 = min([out_1.shape[3], out_2.shape[3]])

            diff2 = (out_1.size(2) - min_shape_2) // 2
            diff3 = (out_1.size(3) - min_shape_3) // 2
            out_1 = out_1[:, :, diff2 : min_shape_2 + diff2, diff3 : min_shape_3 + diff3]

            diff2 = (out_2.size(2) - min_shape_2) // 2
            diff3 = (out_2.size(3) - min_shape_3) // 2
            out_2 = out_2[:, :, diff2: min_shape_2 + diff2, diff3: min_shape_3 + diff3]

        return torch.cat((out_1, out_2), dim=1)  # 通道连接

```

#### Double DIP

虽然论文名叫Double DIP，实际上使用了三个DIP，一个用于生成前景背景分离的mask；一个用于生成前景；一个用于生成背景：

```python
 self.left_net = DIP(out_channels=3).to(device)    
 self.right_net = DIP(out_channels=3).to(device)
 self.mask_net = DIP(out_channels=1).to(device)
```



### 损失函数

Double DIP的可解释性的重点就在于它的损失函数。分为重构损失、正则损失、互斥损失。最后优化的方式是把三者按一定的系数累和，对三个DIP网络的参数进行联合优化。

#### 重构损失

重构损失的直观理解是，三个网络生成的mask、fg、bg图片，能够合成接近于原始图像的图像。在这里直接使用合成图像与原始图像的L1 Loss：

```python
l1_loss = nn.L1Loss().to(device)

def reconst_loss(input_img, recomp_img):
    '''
    重构损失
    :param recomp_img:
    :param self:
    :return:
    '''
    return l1_loss(input_img, recomp_img)
```



#### 正则损失

#### 互斥损失

## 实验结果与分析
